{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTOR CRITIC\n",
    "\n",
    "In this demo lets employ actor-critic method to learn optimal policies.\n",
    "\n",
    "We can summarise the steps of the AC algorithm as follow:\n",
    "\n",
    "1.Produce the action for the current state  (a_t)<br>\n",
    "2.Observe next state (s_t+1) and  and the reward r<br>\n",
    "3.Update the utility of state (s_t)(critic) <br>\n",
    "4.Update the probability of the action using the error (actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "from gridworld import GridWorld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(value_matrix, observation, new_observation, \n",
    "                   reward, alpha, gamma, done):\n",
    "    \n",
    "    u = value_matrix[observation[0], observation[1]]\n",
    "    u_t1 = value_matrix[new_observation[0], new_observation[1]]\n",
    "    delta = reward + ((gamma * u_t1) - u)\n",
    "    value_matrix[observation[0], observation[1]] += alpha * delta\n",
    "    return value_matrix, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_actor(state_action_matrix, observation, action, delta, beta_matrix=None):\n",
    "   \n",
    "    col = observation[1] + (observation[0]*4)\n",
    "    if beta_matrix is None: beta = 1\n",
    "    else: beta = 1 / beta_matrix[action,col]\n",
    "    state_action_matrix[action, col] += beta * delta\n",
    "    return state_action_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Lets create a grid world, the states marked 1 are terminal state and those marked -1 contain obstacles. <br>\n",
    "2.The agent receives a reward of -0.04 for every move from non-terminal states <br>\n",
    "3.The  actions are UP(0), RIGHT(1), DOWN(2) and LEFT(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(3, 4)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "#Define the reward matrix\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action Matrix:\n",
      "[[0.877 0.623 0.738 0.287 0.449 0.701 0.547 0.125 0.596 0.672 0.95  0.988]\n",
      " [0.578 0.422 0.19  0.539 0.258 0.735 0.949 0.284 0.44  0.645 0.407 0.563]\n",
      " [0.084 0.558 0.357 0.663 0.431 0.784 0.354 0.663 0.265 0.199 0.958 0.022]\n",
      " [0.119 0.29  0.228 0.241 0.368 0.774 0.768 0.4   0.732 0.31  0.006 0.724]]\n"
     ]
    }
   ],
   "source": [
    "#Define the transition matrix\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "state_action_matrix = np.random.random((4,12))\n",
    "print(\"State-Action Matrix:\")\n",
    "print(state_action_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Matrix:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "value_matrix = np.zeros((3,4))\n",
    "print(\"Utility Matrix:\")\n",
    "print(value_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.999\n",
    "alpha = 0.001 \n",
    "beta_matrix = np.zeros((4,12))\n",
    "tot_epoch = 30000\n",
    "print_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Value matrix after 1 iterations:\n",
      "[[ 0.     0.    -0.     0.   ]\n",
      " [-0.     0.    -0.     0.   ]\n",
      " [-0.    -0.    -0.    -0.001]]\n",
      "\n",
      "\n",
      "State Value matrix after 1001 iterations:\n",
      "[[-0.021  0.095  0.55   0.   ]\n",
      " [-0.053  0.     0.062  0.   ]\n",
      " [-0.061 -0.058 -0.041 -0.106]]\n",
      "\n",
      "\n",
      "State Value matrix after 2001 iterations:\n",
      "[[ 0.021  0.277  0.764  0.   ]\n",
      " [-0.06   0.     0.214  0.   ]\n",
      " [-0.077 -0.07  -0.009 -0.129]]\n",
      "\n",
      "\n",
      "State Value matrix after 3001 iterations:\n",
      "[[ 0.087  0.435  0.863  0.   ]\n",
      " [-0.05   0.     0.369  0.   ]\n",
      " [-0.082 -0.062  0.062 -0.14 ]]\n",
      "\n",
      "\n",
      "State Value matrix after 4001 iterations:\n",
      "[[ 0.171  0.566  0.904  0.   ]\n",
      " [-0.026  0.     0.502  0.   ]\n",
      " [-0.085 -0.04   0.146 -0.127]]\n",
      "\n",
      "\n",
      "State Value matrix after 5001 iterations:\n",
      "[[ 0.266  0.665  0.927  0.   ]\n",
      " [ 0.008  0.     0.578  0.   ]\n",
      " [-0.083 -0.011  0.231 -0.106]]\n",
      "\n",
      "\n",
      "State Value matrix after 6001 iterations:\n",
      "[[ 0.363  0.74   0.938  0.   ]\n",
      " [ 0.066  0.     0.63   0.   ]\n",
      " [-0.075  0.023  0.302 -0.073]]\n",
      "\n",
      "\n",
      "State Value matrix after 7001 iterations:\n",
      "[[ 0.453  0.793  0.946  0.   ]\n",
      " [ 0.129  0.     0.661  0.   ]\n",
      " [-0.061  0.066  0.364 -0.042]]\n",
      "\n",
      "\n",
      "State Value matrix after 8001 iterations:\n",
      "[[ 0.528  0.83   0.948  0.   ]\n",
      " [ 0.205  0.     0.678  0.   ]\n",
      " [-0.044  0.108  0.408 -0.013]]\n",
      "\n",
      "\n",
      "State Value matrix after 9001 iterations:\n",
      "[[ 0.588  0.855  0.953  0.   ]\n",
      " [ 0.273  0.     0.67   0.   ]\n",
      " [-0.025  0.149  0.441  0.024]]\n",
      "\n",
      "\n",
      "State Value matrix after 10001 iterations:\n",
      "[[0.64  0.873 0.958 0.   ]\n",
      " [0.34  0.    0.681 0.   ]\n",
      " [0.005 0.186 0.464 0.061]]\n",
      "\n",
      "\n",
      "State Value matrix after 11001 iterations:\n",
      "[[0.679 0.885 0.958 0.   ]\n",
      " [0.399 0.    0.68  0.   ]\n",
      " [0.039 0.222 0.488 0.093]]\n",
      "\n",
      "\n",
      "State Value matrix after 12001 iterations:\n",
      "[[0.711 0.893 0.959 0.   ]\n",
      " [0.456 0.    0.696 0.   ]\n",
      " [0.076 0.253 0.508 0.12 ]]\n",
      "\n",
      "\n",
      "State Value matrix after 13001 iterations:\n",
      "[[0.743 0.897 0.957 0.   ]\n",
      " [0.503 0.    0.701 0.   ]\n",
      " [0.107 0.283 0.529 0.141]]\n",
      "\n",
      "\n",
      "State Value matrix after 14001 iterations:\n",
      "[[0.765 0.901 0.959 0.   ]\n",
      " [0.543 0.    0.693 0.   ]\n",
      " [0.142 0.316 0.541 0.161]]\n",
      "\n",
      "\n",
      "State Value matrix after 15001 iterations:\n",
      "[[0.783 0.903 0.959 0.   ]\n",
      " [0.586 0.    0.716 0.   ]\n",
      " [0.18  0.339 0.546 0.172]]\n",
      "\n",
      "\n",
      "State Value matrix after 16001 iterations:\n",
      "[[0.796 0.904 0.959 0.   ]\n",
      " [0.62  0.    0.743 0.   ]\n",
      " [0.219 0.367 0.565 0.201]]\n",
      "\n",
      "\n",
      "State Value matrix after 17001 iterations:\n",
      "[[0.807 0.905 0.958 0.   ]\n",
      " [0.652 0.    0.696 0.   ]\n",
      " [0.255 0.388 0.578 0.221]]\n",
      "\n",
      "\n",
      "State Value matrix after 18001 iterations:\n",
      "[[0.815 0.906 0.956 0.   ]\n",
      " [0.678 0.    0.68  0.   ]\n",
      " [0.289 0.41  0.577 0.242]]\n",
      "\n",
      "\n",
      "State Value matrix after 19001 iterations:\n",
      "[[0.822 0.906 0.96  0.   ]\n",
      " [0.696 0.    0.697 0.   ]\n",
      " [0.322 0.429 0.572 0.257]]\n",
      "\n",
      "\n",
      "State Value matrix after 20001 iterations:\n",
      "[[0.828 0.906 0.954 0.   ]\n",
      " [0.715 0.    0.677 0.   ]\n",
      " [0.353 0.442 0.579 0.274]]\n",
      "\n",
      "\n",
      "State Value matrix after 21001 iterations:\n",
      "[[0.831 0.904 0.954 0.   ]\n",
      " [0.729 0.    0.67  0.   ]\n",
      " [0.382 0.457 0.577 0.291]]\n",
      "\n",
      "\n",
      "State Value matrix after 22001 iterations:\n",
      "[[0.835 0.904 0.955 0.   ]\n",
      " [0.742 0.    0.692 0.   ]\n",
      " [0.41  0.467 0.582 0.306]]\n",
      "\n",
      "\n",
      "State Value matrix after 23001 iterations:\n",
      "[[0.837 0.904 0.954 0.   ]\n",
      " [0.752 0.    0.716 0.   ]\n",
      " [0.443 0.476 0.591 0.314]]\n",
      "\n",
      "\n",
      "State Value matrix after 24001 iterations:\n",
      "[[0.838 0.905 0.959 0.   ]\n",
      " [0.76  0.    0.716 0.   ]\n",
      " [0.47  0.488 0.595 0.326]]\n",
      "\n",
      "\n",
      "State Value matrix after 25001 iterations:\n",
      "[[0.838 0.905 0.958 0.   ]\n",
      " [0.766 0.    0.719 0.   ]\n",
      " [0.494 0.497 0.602 0.335]]\n",
      "\n",
      "\n",
      "State Value matrix after 26001 iterations:\n",
      "[[0.841 0.906 0.961 0.   ]\n",
      " [0.772 0.    0.719 0.   ]\n",
      " [0.511 0.507 0.604 0.35 ]]\n",
      "\n",
      "\n",
      "State Value matrix after 27001 iterations:\n",
      "[[0.842 0.907 0.96  0.   ]\n",
      " [0.775 0.    0.716 0.   ]\n",
      " [0.532 0.514 0.605 0.367]]\n",
      "\n",
      "\n",
      "State Value matrix after 28001 iterations:\n",
      "[[0.845 0.909 0.965 0.   ]\n",
      " [0.779 0.    0.706 0.   ]\n",
      " [0.547 0.521 0.614 0.378]]\n",
      "\n",
      "\n",
      "State Value matrix after 29001 iterations:\n",
      "[[0.846 0.908 0.957 0.   ]\n",
      " [0.783 0.    0.714 0.   ]\n",
      " [0.563 0.528 0.612 0.381]]\n",
      "\n",
      "value matrix after 30000 iterations:\n",
      "[[0.847 0.907 0.957 0.   ]\n",
      " [0.786 0.    0.701 0.   ]\n",
      " [0.579 0.534 0.616 0.383]]\n",
      "State-Action matrix after  30000 iterations:\n",
      "[[ -6.376  -0.599   0.658   0.287 834.358   0.701 724.009   0.125 644.59\n",
      "  -17.799 649.591 -37.335]\n",
      " [867.625 910.992 957.715   0.539 -15.299   0.735  -9.006   0.284 -20.056\n",
      "  588.557 -11.107 -36.118]\n",
      " [ -6.503  -0.868   0.108   0.663 -15.76    0.784  -6.403   0.663 -21.688\n",
      "  -17.524 -10.293 -32.902]\n",
      " [ -6.452  -1.      0.147   0.241 -15.604   0.774  -4.908   0.4   -21.583\n",
      "  -17.903 -10.326 491.453]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(tot_epoch):\n",
    "    #Reset and return the first observation\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    for step in range(1000):\n",
    "        #Estimating the action through Softmax\n",
    "        col = observation[1] + (observation[0]*4)\n",
    "        action_array = state_action_matrix[:, col]\n",
    "        action_distribution = softmax(action_array)\n",
    "        action = np.random.choice(4, 1, p=action_distribution)\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        value_matrix, delta = update_critic(value_matrix, observation, \n",
    "                                              new_observation, reward, alpha, gamma, done)\n",
    "        state_action_matrix = update_actor(state_action_matrix, observation, \n",
    "                                           action, delta, beta_matrix=None)\n",
    "        observation = new_observation\n",
    "        if done: break\n",
    "\n",
    "\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        print(\"State Value matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(value_matrix)\n",
    "        print(\"\")\n",
    "        #print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        #print(state_action_matrix)\n",
    "#Time to check the utility matrix obtained\n",
    "print(\"value matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(value_matrix)\n",
    "print(\"State-Action matrix after  \" + str(tot_epoch) + \" iterations:\")\n",
    "print(state_action_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
