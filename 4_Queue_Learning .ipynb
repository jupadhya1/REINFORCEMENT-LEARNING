{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "In this demo lets employ q-learning to find optimal policy where we use exploratory policy (ep) to update a random policy to make it optimal(pi).\n",
    "\n",
    "1.Take an action according to exploratory policy(ep)<br>\n",
    "2.Observe (s_t+1) and reward(r)<br>\n",
    "3.Update the state-action function<br> \n",
    "4.Update the optimal policy(pi)<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "class GridWorld:\n",
    "\n",
    "    def __init__(self, tot_row, tot_col):\n",
    "        self.action_space_size = 4\n",
    "        self.world_row = tot_row\n",
    "        self.world_col = tot_col\n",
    "        self.transition_matrix = np.ones((self.action_space_size, self.action_space_size))/ self.action_space_size\n",
    "        self.reward_matrix = np.zeros((tot_row, tot_col))\n",
    "        self.state_matrix = np.zeros((tot_row, tot_col))\n",
    "        self.position = [np.random.randint(tot_row), np.random.randint(tot_col)]\n",
    "\n",
    "\n",
    "    def setTransitionMatrix(self, transition_matrix):\n",
    "        if(transition_matrix.shape != self.transition_matrix.shape):\n",
    "            raise ValueError('The shape of the two matrices must be the same.') \n",
    "        self.transition_matrix = transition_matrix\n",
    "\n",
    "    def setRewardMatrix(self, reward_matrix):\n",
    "        if(reward_matrix.shape != self.reward_matrix.shape):\n",
    "            raise ValueError('The shape of the matrix does not match with the shape of the world.')\n",
    "        self.reward_matrix = reward_matrix\n",
    "\n",
    "    def setStateMatrix(self, state_matrix):\n",
    "        '''Set the obstacles in the world.\n",
    "\n",
    "        The input to the function is a matrix with the\n",
    "        same size of the world \n",
    "        -1 for states which are not walkable.\n",
    "        +1 for terminal states\n",
    "         0 for all the walkable states (non terminal)\n",
    "        '''\n",
    "        if(state_matrix.shape != self.state_matrix.shape):\n",
    "            raise ValueError('The shape of the matrix does not match with the shape of the world.')\n",
    "        self.state_matrix = state_matrix\n",
    "\n",
    "    def setPosition(self, index_row=None, index_col=None):\n",
    "        if(index_row is None or index_col is None): self.position = [np.random.randint(tot_row), np.random.randint(tot_col)]\n",
    "        else: self.position = [index_row, index_col]\n",
    "\n",
    "    def render(self):\n",
    "        ''' Print the current world in the terminal.\n",
    "\n",
    "        O represents the robot position\n",
    "        - respresent empty states.\n",
    "        # represents obstacles\n",
    "        * represents terminal states\n",
    "        '''\n",
    "        graph = \"\"\n",
    "        for row in range(self.world_row):\n",
    "            row_string = \"\"\n",
    "            for col in range(self.world_col):\n",
    "                if(self.position == [row, col]): row_string += u\" \\u25CB \" # u\" \\u25CC \"\n",
    "                else:\n",
    "                    if(self.state_matrix[row, col] == 0): row_string += ' - '\n",
    "                    elif(self.state_matrix[row, col] == -1): row_string += ' # '\n",
    "                    elif(self.state_matrix[row, col] == +1): row_string += ' * '\n",
    "            row_string += '\\n'\n",
    "            graph += row_string \n",
    "        print(graph)            \n",
    "\n",
    "    def reset(self, exploring_starts=False):\n",
    "        ''' Set the position of the robot in the bottom left corner.\n",
    "\n",
    "        It returns the first observation\n",
    "        '''\n",
    "        if exploring_starts:\n",
    "            while(True):\n",
    "                row = np.random.randint(0, self.world_row)\n",
    "                col = np.random.randint(0, self.world_col)\n",
    "                if(self.state_matrix[row, col] == 0): break\n",
    "            self.position = [row, col]\n",
    "        else:\n",
    "            self.position = [self.world_row-1, 0]\n",
    "        #reward = self.reward_matrix[self.position[0], self.position[1]]\n",
    "        return self.position\n",
    "\n",
    "    def step(self, action):\n",
    "        ''' One step in the world.\n",
    "\n",
    "        [observation, reward, done = env.step(action)]\n",
    "        '''\n",
    "        if(action >= self.action_space_size): \n",
    "            raise ValueError('The action is not included in the action space.')\n",
    "\n",
    "        #Based on the current action and the probability derived\n",
    "        #from the trasition model it chooses a new actio to perform\n",
    "        action = np.random.choice(4, 1, p=self.transition_matrix[int(action),:])\n",
    "        #action = self.transition_model(action)\n",
    "\n",
    "        #Generating a new position based on the current position and action\n",
    "        if(action == 0): new_position = [self.position[0]-1, self.position[1]]   #UP\n",
    "        elif(action == 1): new_position = [self.position[0], self.position[1]+1] #RIGHT\n",
    "        elif(action == 2): new_position = [self.position[0]+1, self.position[1]] #DOWN\n",
    "        elif(action == 3): new_position = [self.position[0], self.position[1]-1] #LEFT\n",
    "        else: raise ValueError('The action is not included in the action space.')\n",
    "\n",
    "        #Check if the new position is a valid position\n",
    "        #print(self.state_matrix)\n",
    "        if (new_position[0]>=0 and new_position[0]<self.world_row):\n",
    "            if(new_position[1]>=0 and new_position[1]<self.world_col):\n",
    "                if(self.state_matrix[new_position[0], new_position[1]] != -1):\n",
    "                    self.position = new_position\n",
    "\n",
    "        reward = self.reward_matrix[self.position[0], self.position[1]]\n",
    "        #Done is True if the state is a terminal state\n",
    "        done = bool(self.state_matrix[self.position[0], self.position[1]])\n",
    "        return self.position, reward, done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state_action(state_action_matrix,  observation, new_observation,\n",
    "                        action, reward, alpha, gamma):\n",
    "    '''Return the updated utility matrix\n",
    "    '''\n",
    "    #Getting the values of Q at t and at t+1\n",
    "    col = observation[1] + (observation[0]*4)\n",
    "    q = state_action_matrix[action ,col]\n",
    "    col_t1 = new_observation[1] + (new_observation[0]*4)\n",
    "    q_t1 = np.max(state_action_matrix[: ,col_t1])\n",
    "    #Calculate alpha based on how many time it\n",
    "    #has been visited\n",
    "    #alpha_counted = 1.0 / (1.0 + visit_counter_matrix[action, col])\n",
    "    #Applying the update rule\n",
    "    #Here you can change \"alpha\" with \"alpha_counted\" if you want\n",
    "    #to take into account how many times that particular state-action\n",
    "    #pair has been visited until now.\n",
    "    state_action_matrix[action ,col] = state_action_matrix[action ,col] + alpha * (reward + gamma * q_t1 - q)\n",
    "    return state_action_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy_matrix, state_action_matrix, observation):\n",
    "    '''Return the updated policy matrix (q-learning)\n",
    "    '''\n",
    "    col = observation[1] + (observation[0]*4)\n",
    "    #Getting the index of the action with the highest utility\n",
    "    best_action = np.argmax(state_action_matrix[:, col])\n",
    "    #Updating the policy\n",
    "    policy_matrix[observation[0], observation[1]] = best_action\n",
    "    return policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_epsilon_greedy_action(policy_matrix, observation, epsilon=0.1):\n",
    "    tot_actions = int(np.nanmax(policy_matrix) + 1)\n",
    "    action = int(policy_matrix[observation[0], observation[1]])\n",
    "    non_greedy_prob = epsilon / tot_actions\n",
    "    greedy_prob = 1 - epsilon + non_greedy_prob\n",
    "    weight_array = np.full((tot_actions), non_greedy_prob)\n",
    "    weight_array[action] = greedy_prob\n",
    "    return np.random.choice(tot_actions, 1, p=weight_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_decayed_value(starting_value, global_step, decay_step):\n",
    "        \"\"\"Returns the decayed value.\n",
    "\n",
    "        decayed_value = starting_value * decay_rate ^ (global_step / decay_steps)\n",
    "        @param starting_value the value before decaying\n",
    "        @param global_step the global step to use for decay (positive integer)\n",
    "        @param decay_step the step at which the value is decayed\n",
    "        \"\"\"\n",
    "        decayed_value = starting_value * np.power(0.1, (global_step/decay_step))\n",
    "        return decayed_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(3, 4)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "#Define the reward matrix\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the transition matrix\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Matrix:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  1. -1.]\n",
      " [ 1.  3.  3.  2.]]\n"
     ]
    }
   ],
   "source": [
    "#Random policy\n",
    "policy_matrix = np.random.randint(low=0, high=4, size=(3, 4)).astype(np.float32)\n",
    "policy_matrix[1,1] = np.NaN #NaN for the obstacle at (1,1)\n",
    "policy_matrix[0,3] = policy_matrix[1,3] = -1 #No action for the terminal states\n",
    "print(\"Policy Matrix:\")\n",
    "print(policy_matrix)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploratory Policy Matrix:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  1.  0.  3.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exploratory_policy_matrix = np.array([[1,      1, 1, -1],\n",
    "                                      [0, np.NaN, 0, -1],\n",
    "                                      [0,      1, 0,  3]])\n",
    "\n",
    "print(\"Exploratory Policy Matrix:\")\n",
    "print(exploratory_policy_matrix)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "state_action_matrix = np.zeros((4,12))\n",
    "visit_counter_matrix = np.zeros((4,12))\n",
    "gamma = 0.999\n",
    "alpha = 0.001 #constant step size\n",
    "tot_epoch = 5000\n",
    "print_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epsilon: 0.1\n",
      "State-Action matrix after 1 iterations:\n",
      "[[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.001  0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.    -0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]]\n",
      "Policy matrix after 1 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  1. -1.]\n",
      " [ 1.  3.  3.  2.]]\n",
      "\n",
      "Epsilon: 0.0954992586021436\n",
      "State-Action matrix after 1001 iterations:\n",
      "[[-0.001 -0.     0.007  0.    -0.013  0.     0.05   0.    -0.006 -0.001\n",
      "  -0.009 -0.022]\n",
      " [-0.002  0.116  0.553  0.    -0.001  0.    -0.026  0.    -0.001 -0.006\n",
      "  -0.001 -0.001]\n",
      " [-0.001  0.001  0.     0.    -0.001  0.    -0.005  0.    -0.001 -0.001\n",
      "  -0.001 -0.001]\n",
      " [-0.001 -0.001  0.001  0.    -0.001  0.     0.     0.    -0.001 -0.001\n",
      "  -0.001 -0.021]]\n",
      "Policy matrix after 1001 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  0.  3.  2.]]\n",
      "\n",
      "Epsilon: 0.09120108393559098\n",
      "State-Action matrix after 2001 iterations:\n",
      "[[-0.001  0.005  0.025  0.    -0.016  0.     0.202  0.    -0.011 -0.002\n",
      "   0.013 -0.046]\n",
      " [ 0.046  0.305  0.765  0.    -0.002  0.    -0.046  0.    -0.002 -0.012\n",
      "  -0.002 -0.002]\n",
      " [-0.002  0.007  0.007  0.    -0.002  0.    -0.009  0.    -0.003 -0.002\n",
      "  -0.002 -0.002]\n",
      " [-0.002  0.     0.007  0.    -0.003  0.     0.003  0.    -0.002 -0.002\n",
      "  -0.002 -0.041]]\n",
      "Policy matrix after 2001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 1. nan  0. -1.]\n",
      " [ 1.  2.  0.  2.]]\n",
      "\n",
      "Epsilon: 0.08709635899560808\n",
      "State-Action matrix after 3001 iterations:\n",
      "[[ 0.     0.015  0.048  0.    -0.002  0.     0.339  0.    -0.016 -0.003\n",
      "   0.071 -0.061]\n",
      " [ 0.132  0.472  0.853  0.    -0.003  0.    -0.061  0.    -0.003 -0.012\n",
      "  -0.002 -0.005]\n",
      " [-0.001  0.016  0.015  0.    -0.004  0.    -0.013  0.    -0.003 -0.003\n",
      "  -0.003 -0.003]\n",
      " [-0.001  0.003  0.014  0.    -0.004  0.     0.013  0.    -0.003 -0.003\n",
      "  -0.003 -0.051]]\n",
      "Policy matrix after 3001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 1.  0.  0.  2.]]\n",
      "\n",
      "Epsilon: 0.0831763771102671\n",
      "State-Action matrix after 4001 iterations:\n",
      "[[ 0.003  0.034  0.066  0.     0.031  0.     0.466  0.    -0.018 -0.003\n",
      "   0.142 -0.089]\n",
      " [ 0.235  0.605  0.894  0.    -0.003  0.    -0.08   0.    -0.004 -0.001\n",
      "  -0.002 -0.007]\n",
      " [ 0.002  0.029  0.025  0.    -0.004  0.    -0.012  0.    -0.005 -0.003\n",
      "  -0.001 -0.003]\n",
      " [ 0.002  0.011  0.028  0.    -0.004  0.     0.022  0.    -0.004 -0.004\n",
      "  -0.003 -0.047]]\n",
      "Policy matrix after 4001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 3.  1.  0.  2.]]\n",
      "State-Action matrix after 5000 iterations:\n",
      "[[ 0.008  0.05   0.086  0.     0.078  0.     0.559  0.    -0.015 -0.003\n",
      "   0.223 -0.099]\n",
      " [ 0.335  0.694  0.922  0.    -0.003  0.    -0.098  0.    -0.005  0.021\n",
      "   0.002 -0.013]\n",
      " [ 0.003  0.044  0.044  0.    -0.005  0.    -0.01   0.    -0.005 -0.004\n",
      "   0.002 -0.004]\n",
      " [ 0.009  0.019  0.043  0.    -0.002  0.     0.036  0.    -0.005 -0.005\n",
      "  -0.001 -0.04 ]]\n",
      "Policy matrix after 5000 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 3.  1.  0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(tot_epoch):\n",
    "    #Reset and return the first observation\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    epsilon = return_decayed_value(0.1, epoch, decay_step=50000)\n",
    "    is_starting = True\n",
    "    \n",
    "    for step in range(1000):\n",
    "        #Take the action using epsilon-greedy\n",
    "        action = return_epsilon_greedy_action(exploratory_policy_matrix, observation, epsilon=0.001)\n",
    "        if(is_starting):\n",
    "            action = np.random.randint(0, 4)\n",
    "            is_starting = False\n",
    "        #Move one step in the environment and get obs and reward\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        #Updating the state-action matrix\n",
    "        state_action_matrix = update_state_action(state_action_matrix,observation, new_observation,\n",
    "                                                  action, reward, alpha, gamma)\n",
    "        #Updating the policy\n",
    "        policy_matrix = update_policy(policy_matrix, state_action_matrix, observation)\n",
    "        observation = new_observation\n",
    "        if done: break\n",
    "\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        print(\"Epsilon: \" + str(epsilon))\n",
    "        print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\")\n",
    "        print(state_action_matrix)\n",
    "        print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\")\n",
    "        print(policy_matrix)\n",
    "\n",
    "#Time to check the utility matrix obtained\n",
    "print(\"State-Action matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(state_action_matrix)\n",
    "print(\"Policy matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(policy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
